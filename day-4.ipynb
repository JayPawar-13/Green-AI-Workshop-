{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4932759,"sourceType":"datasetVersion","datasetId":2860500}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #for data augumentation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nimport tkinter as tk\nfrom tkinter import filedialog #provides a dialog box to select files.\nfrom PIL import Image, ImageTk #ImageTk - Converts images for display in Tkinter GUIs\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:07:22.256732Z","iopub.execute_input":"2025-01-17T08:07:22.257179Z","iopub.status.idle":"2025-01-17T08:07:42.707280Z","shell.execute_reply.started":"2025-01-17T08:07:22.257042Z","shell.execute_reply":"2025-01-17T08:07:42.706328Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Set up directories\ntrain_dir = r\"/kaggle/input/wildfire-prediction-dataset/train\"\nvalid_dir = r\"/kaggle/input/wildfire-prediction-dataset/valid\"\ntest_dir = r\"/kaggle/input/wildfire-prediction-dataset/test\"\n\n# Set up ImageDataGenerators for loading images\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalid_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Load images from directories\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\nvalid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\ntest_generator = test_datagen.flow_from_directory(test_dir, target_size=(64, 64), batch_size=32, class_mode='binary')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:08:21.270949Z","iopub.execute_input":"2025-01-17T08:08:21.271587Z","iopub.status.idle":"2025-01-17T08:08:50.212637Z","shell.execute_reply.started":"2025-01-17T08:08:21.271556Z","shell.execute_reply":"2025-01-17T08:08:50.211656Z"}},"outputs":[{"name":"stdout","text":"Found 30250 images belonging to 2 classes.\nFound 6300 images belonging to 2 classes.\nFound 6300 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Building a simple CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Binary classification: wildfire or no wildfire\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:09:07.311525Z","iopub.execute_input":"2025-01-17T08:09:07.311810Z","iopub.status.idle":"2025-01-17T08:09:07.361670Z","shell.execute_reply.started":"2025-01-17T08:09:07.311791Z","shell.execute_reply":"2025-01-17T08:09:07.360527Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:09:28.203852Z","iopub.execute_input":"2025-01-17T08:09:28.204141Z","iopub.status.idle":"2025-01-17T08:09:28.207503Z","shell.execute_reply.started":"2025-01-17T08:09:28.204122Z","shell.execute_reply":"2025-01-17T08:09:28.206662Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_generator, validation_data=valid_generator, epochs=2, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:09:51.652541Z","iopub.execute_input":"2025-01-17T08:09:51.652841Z","iopub.status.idle":"2025-01-17T08:15:02.147590Z","shell.execute_reply.started":"2025-01-17T08:09:51.652821Z","shell.execute_reply":"2025-01-17T08:15:02.146705Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 224ms/step - accuracy: 0.8749 - loss: 0.2995 - val_accuracy: 0.9397 - val_loss: 0.1678\nEpoch 2/2\n\u001b[1m946/946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 103ms/step - accuracy: 0.9215 - loss: 0.2113 - val_accuracy: 0.9400 - val_loss: 0.1663\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.save(\"ffd_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:18:54.734470Z","iopub.execute_input":"2025-01-17T08:18:54.734765Z","iopub.status.idle":"2025-01-17T08:18:54.814103Z","shell.execute_reply.started":"2025-01-17T08:18:54.734744Z","shell.execute_reply":"2025-01-17T08:18:54.813356Z"}},"outputs":[],"execution_count":23}]}